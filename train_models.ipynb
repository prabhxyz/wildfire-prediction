{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Very rough code for a wildfire prediction model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d911c43c500c9bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import the libraries. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a8871a12bc6c2c9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Optional, Text, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56e12d6ec726c216",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9203ae1608d9b5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "file_pattern = 'wildfire_data/next_day_wildfire_spread_train*'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42b292c3d2ee23cc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"Constants for the data reader.\"\"\"\n",
    "\n",
    "INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph', \n",
    "                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
    "\n",
    "OUTPUT_FEATURES = ['FireMask', ]\n",
    "\n",
    "# Data statistics \n",
    "# For each variable, the statistics are ordered in the form:\n",
    "# (min_clip, max_clip, mean, standard deviation)\n",
    "DATA_STATS = {\n",
    "    # Elevation in m.\n",
    "    # 0.1 percentile, 99.9 percentile\n",
    "    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n",
    "    # Pressure\n",
    "    # 0.1 percentile, 99.9 percentile\n",
    "    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n",
    "    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),  # min, max\n",
    "    # Precipitation in mm.\n",
    "    # Negative values do not make sense, so min is set to 0.\n",
    "    # 0., 99.9 percentile\n",
    "    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n",
    "    # Specific humidity.\n",
    "    # Negative values do not make sense, so min is set to 0.\n",
    "    # The range of specific humidity is up to 100% so max is 1.\n",
    "    'sph': (0., 1., 0.0071658953, 0.0042835088),\n",
    "    # Wind direction in degrees clockwise from north.\n",
    "    # Thus min set to 0 and max set to 360.\n",
    "    'th': (0., 360.0, 190.32976, 72.59854),\n",
    "    # Min/max temperature in Kelvin.\n",
    "    # -20 degree C, 99.9 percentile\n",
    "    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n",
    "    # -20 degree C, 99.9 percentile\n",
    "    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n",
    "    # Wind speed in m/s.\n",
    "    # Negative values do not make sense, given there is a wind direction.\n",
    "    # 0., 99.9 percentile\n",
    "    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n",
    "    # NFDRS fire danger index energy release component expressed in BTU's per\n",
    "    # square foot.\n",
    "    # Negative values do not make sense. Thus min set to zero.\n",
    "    # 0., 99.9 percentile\n",
    "    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n",
    "    # Population density\n",
    "    # min, 99.9 percentile\n",
    "    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n",
    "    # We don't want to normalize the FireMasks.\n",
    "    # 1 indicates fire, 0 no fire, -1 unlabeled data\n",
    "    'PrevFireMask': (-1., 1., 0., 1.),\n",
    "    'FireMask': (-1., 1., 0., 1.)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5707312fb4950fb2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cropping function to extract regions of the input image."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da400e6ca608b1de"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"Library of common functions used in deep learning neural networks.\n",
    "\"\"\"\n",
    "#YOU PROBABLY WILL NOT USE THESE.\n",
    "\n",
    "def random_crop_input_and_output_images(\n",
    "    input_img: tf.Tensor,\n",
    "    output_img: tf.Tensor,\n",
    "    sample_size: int,\n",
    "    num_in_channels: int,\n",
    "    num_out_channels: int,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Randomly axis-align crop input and output image tensors.\n",
    "\n",
    "  Args:\n",
    "    input_img: tensor with dimensions HWC.\n",
    "    output_img: tensor with dimensions HWC.\n",
    "    sample_size: side length (square) to crop to.\n",
    "    num_in_channels: number of channels in input_img.\n",
    "    num_out_channels: number of channels in output_img.\n",
    "  Returns:\n",
    "    input_img: tensor with dimensions HWC.\n",
    "    output_img: tensor with dimensions HWC.\n",
    "  \"\"\"\n",
    "  combined = tf.concat([input_img, output_img], axis=2)\n",
    "  combined = tf.image.random_crop(\n",
    "      combined,\n",
    "      [sample_size, sample_size, num_in_channels + num_out_channels])\n",
    "  input_img = combined[:, :, 0:num_in_channels]\n",
    "  output_img = combined[:, :, -num_out_channels:]\n",
    "  return input_img, output_img\n",
    "\n",
    "\n",
    "def center_crop_input_and_output_images(\n",
    "    input_img: tf.Tensor,\n",
    "    output_img: tf.Tensor,\n",
    "    sample_size: int,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Center crops input and output image tensors.\n",
    "\n",
    "  Args:\n",
    "    input_img: tensor with dimensions HWC.\n",
    "    output_img: tensor with dimensions HWC.\n",
    "    sample_size: side length (square) to crop to.\n",
    "  Returns:\n",
    "    input_img: tensor with dimensions HWC.\n",
    "    output_img: tensor with dimensions HWC.\n",
    "  \"\"\"\n",
    "  central_fraction = sample_size / input_img.shape[0]\n",
    "  input_img = tf.image.central_crop(input_img, central_fraction)\n",
    "  output_img = tf.image.central_crop(output_img, central_fraction)\n",
    "  return input_img, output_img"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ff9ab4e5477458f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset reader. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34a91b6ebe7f23e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"Dataset reader for Earth Engine data.\"\"\"\n",
    "\n",
    "def _get_base_key(key: Text) -> Text:\n",
    "  \"\"\"Extracts the base key from the provided key.\n",
    "\n",
    "  Earth Engine exports TFRecords containing each data variable with its\n",
    "  corresponding variable name. In the case of time sequences, the name of the\n",
    "  data variable is of the form 'variable_1', 'variable_2', ..., 'variable_n',\n",
    "  where 'variable' is the name of the variable, and n the number of elements\n",
    "  in the time sequence. Extracting the base key ensures that each step of the\n",
    "  time sequence goes through the same normalization steps.\n",
    "  The base key obeys the following naming pattern: '([a-zA-Z]+)'\n",
    "  For instance, for an input key 'variable_1', this function returns 'variable'.\n",
    "  For an input key 'variable', this function simply returns 'variable'.\n",
    "\n",
    "  Args:\n",
    "    key: Input key.\n",
    "\n",
    "  Returns:\n",
    "    The corresponding base key.\n",
    "\n",
    "  Raises:\n",
    "    ValueError when `key` does not match the expected pattern.\n",
    "  \"\"\"\n",
    "  match = re.match(r'([a-zA-Z]+)', key)\n",
    "  if match:\n",
    "    return match.group(1)\n",
    "  raise ValueError(\n",
    "      'The provided key does not match the expected pattern: {}'.format(key))\n",
    "\n",
    "\n",
    "def _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
    "  \"\"\"Clips and rescales inputs with the stats corresponding to `key`.\n",
    "\n",
    "  Args:\n",
    "    inputs: Inputs to clip and rescale.\n",
    "    key: Key describing the inputs.\n",
    "\n",
    "  Returns:\n",
    "    Clipped and rescaled input.\n",
    "\n",
    "  Raises:\n",
    "    ValueError if there are no data statistics available for `key`.\n",
    "  \"\"\"\n",
    "  base_key = _get_base_key(key)\n",
    "  if base_key not in DATA_STATS:\n",
    "    raise ValueError(\n",
    "        'No data statistics available for the requested key: {}.'.format(key))\n",
    "  min_val, max_val, _, _ = DATA_STATS[base_key]\n",
    "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
    "  return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n",
    "\n",
    "\n",
    "def _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
    "  \"\"\"Clips and normalizes inputs with the stats corresponding to `key`.\n",
    "\n",
    "  Args:\n",
    "    inputs: Inputs to clip and normalize.\n",
    "    key: Key describing the inputs.\n",
    "\n",
    "  Returns:\n",
    "    Clipped and normalized input.\n",
    "\n",
    "  Raises:\n",
    "    ValueError if there are no data statistics available for `key`.\n",
    "  \"\"\"\n",
    "  base_key = _get_base_key(key)\n",
    "  if base_key not in DATA_STATS:\n",
    "    raise ValueError(\n",
    "        'No data statistics available for the requested key: {}.'.format(key))\n",
    "  min_val, max_val, mean, std = DATA_STATS[base_key]\n",
    "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
    "  inputs = inputs - mean\n",
    "  return tf.math.divide_no_nan(inputs, std)\n",
    "\n",
    "def _get_features_dict(\n",
    "    sample_size: int,\n",
    "    features: List[Text],\n",
    ") -> Dict[Text, tf.io.FixedLenFeature]:\n",
    "  \"\"\"Creates a features dictionary for TensorFlow IO.\n",
    "\n",
    "  Args:\n",
    "    sample_size: Size of the input tiles (square).\n",
    "    features: List of feature names.\n",
    "\n",
    "  Returns:\n",
    "    A features dictionary for TensorFlow IO.\n",
    "  \"\"\"\n",
    "  sample_shape = [sample_size, sample_size]\n",
    "  features = set(features)\n",
    "  columns = [\n",
    "      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n",
    "      for _ in features\n",
    "  ]\n",
    "  return dict(zip(features, columns))\n",
    "\n",
    "\n",
    "def _parse_fn(\n",
    "    example_proto: tf.train.Example, data_size: int, sample_size: int,\n",
    "    num_in_channels: int, clip_and_normalize: bool,\n",
    "    clip_and_rescale: bool, random_crop: bool, center_crop: bool,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Reads a serialized example.\n",
    "\n",
    "  Args:\n",
    "    example_proto: A TensorFlow example protobuf.\n",
    "    data_size: Size of tiles (square) as read from input files.\n",
    "    sample_size: Size the tiles (square) when input into the model.\n",
    "    num_in_channels: Number of input channels.\n",
    "    clip_and_normalize: True if the data should be clipped and normalized.\n",
    "    clip_and_rescale: True if the data should be clipped and rescaled.\n",
    "    random_crop: True if the data should be randomly cropped.\n",
    "    center_crop: True if the data should be cropped in the center.\n",
    "\n",
    "  Returns:\n",
    "    (input_img, output_img) tuple of inputs and outputs to the ML model.\n",
    "  \"\"\"\n",
    "  if (random_crop and center_crop):\n",
    "    raise ValueError('Cannot have both random_crop and center_crop be True')\n",
    "  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n",
    "  feature_names = input_features + output_features\n",
    "  features_dict = _get_features_dict(data_size, feature_names)\n",
    "  features = tf.io.parse_single_example(example_proto, features_dict)\n",
    "\n",
    "  if clip_and_normalize:\n",
    "    inputs_list = [\n",
    "        _clip_and_normalize(features.get(key), key) for key in input_features\n",
    "    ]\n",
    "  elif clip_and_rescale:\n",
    "    inputs_list = [\n",
    "        _clip_and_rescale(features.get(key), key) for key in input_features\n",
    "    ]\n",
    "  else:\n",
    "    inputs_list = [features.get(key) for key in input_features]\n",
    "  \n",
    "  inputs_stacked = tf.stack(inputs_list, axis=0)\n",
    "  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n",
    "\n",
    "  outputs_list = [features.get(key) for key in output_features]\n",
    "  assert outputs_list, 'outputs_list should not be empty'\n",
    "  outputs_stacked = tf.stack(outputs_list, axis=0)\n",
    "\n",
    "  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n",
    "  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n",
    "                                            'but dimensions of outputs_stacked'\n",
    "                                            f' are {outputs_stacked_shape}')\n",
    "  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n",
    "\n",
    "  if random_crop:\n",
    "    input_img, output_img = random_crop_input_and_output_images(\n",
    "        input_img, output_img, sample_size, num_in_channels, 1)\n",
    "  if center_crop:\n",
    "    input_img, output_img = center_crop_input_and_output_images(\n",
    "        input_img, output_img, sample_size)\n",
    "  return input_img, output_img\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n",
    "                batch_size: int, num_in_channels: int, compression_type: Text,\n",
    "                clip_and_normalize: bool, clip_and_rescale: bool,\n",
    "                random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n",
    "  \"\"\"Gets the dataset from the file pattern.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: Input file pattern.\n",
    "    data_size: Size of tiles (square) as read from input files.\n",
    "    sample_size: Size the tiles (square) when input into the model.\n",
    "    batch_size: Batch size.\n",
    "    num_in_channels: Number of input channels.\n",
    "    compression_type: Type of compression used for the input files.\n",
    "    clip_and_normalize: True if the data should be clipped and normalized, False\n",
    "      otherwise.\n",
    "    clip_and_rescale: True if the data should be clipped and rescaled, False\n",
    "      otherwise.\n",
    "    random_crop: True if the data should be randomly cropped.\n",
    "    center_crop: True if the data shoulde be cropped in the center.\n",
    "\n",
    "  Returns:\n",
    "    A TensorFlow dataset loaded from the input file pattern, with features\n",
    "    described in the constants, and with the shapes determined from the input\n",
    "    parameters to this function.\n",
    "  \"\"\"\n",
    "  if (clip_and_normalize and clip_and_rescale):\n",
    "    raise ValueError('Cannot have both normalize and rescale.')\n",
    "  dataset = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = dataset.interleave(\n",
    "      lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.map(\n",
    "      lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n",
    "          x, data_size, sample_size, num_in_channels, clip_and_normalize,\n",
    "          clip_and_rescale, random_crop, center_crop),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  return dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8403a598bd534a4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1c4ca324fa0c6c7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "side_length = 32 #length of the side of the square you select (so, e.g. pick 64 if you don't want any random cropping)\n",
    "num_obs = 100 #batch size\n",
    "\n",
    "dataset = get_dataset(\n",
    "      file_pattern,\n",
    "      data_size=64,\n",
    "      sample_size=side_length,\n",
    "      batch_size=num_obs,\n",
    "      num_in_channels=12,\n",
    "      compression_type=None,\n",
    "      clip_and_normalize=False,\n",
    "      clip_and_rescale=False,\n",
    "      random_crop=True,\n",
    "      center_crop=False) ##########################################################################################################"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba198cdc9ab4ebfb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fetch the first batch of data from the dataset. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a6a7ecdfdf37bef"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(dataset)) \n",
    "#Are there two assignments happening on every iteration because dataset stores inputs with labels?\n",
    "#print(inputs.shape) #(100, 32, 32, 12)\n",
    "#print(labels.shape) #(100, 32, 32, 1)\n",
    "#print(inputs[0, :, :, 11]) #Trying to grab the previous fire mask. (Apparent) success!\n",
    "#print(labels[0,:, :, 0]) #Ok, I think the labels are the fire mask. (That also accords with standard usage of the term.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b8999c09096ad9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consider avg neighbor fire scores. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98d93299834d1907"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Eventually would like a function that takes in an input array of dimensions nxn, \n",
    "#outputs an array that gives avg of each cell's neighbors:\n",
    "def avg_neighbors(array_in):\n",
    "    #Check input\n",
    "    if array_in.shape[0] != array_in.shape[1]:\n",
    "        raise Exception('Only square arrays make sense here, since you\\'re analyzing square arrays.')\n",
    "    #Maybe should also do type-checking, but leave it for now.\n",
    "    \n",
    "    #Prepare the output array:\n",
    "    n = array_in.shape[0] \n",
    "    array_out = np.zeros((n,n))\n",
    "    \n",
    "    #Guess who doesn't know how to do signal processing in Python...\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == 0:\n",
    "                #Upper edge\n",
    "                if j == 0:\n",
    "                    #Upper left corner\n",
    "                    sum_neighbors = array_in[i+1, j] + array_in[i, j+1] + array_in[i+1, j+1]\n",
    "                    avg = sum_neighbors/3\n",
    "                    \n",
    "                elif j == (n-1):\n",
    "                    #Upper right corner\n",
    "                    sum_neighbors = array_in[i, j-1] + array_in[i+1, j] + array_in[i+1, j-1]\n",
    "                    avg = sum_neighbors/3\n",
    "                    \n",
    "                else:\n",
    "                    #Upper edge except corners\n",
    "                    sum_neighbors = array_in[i, j-1] + array_in[i+1, j] + array_in[i, j+1] + array_in[i+1, j-1] + array_in[i+1, j+1]\n",
    "                    avg = sum_neighbors/5\n",
    "                   \n",
    "            elif i == (n-1):\n",
    "                #Lower edge\n",
    "                if j == 0:\n",
    "                    #Lower left corner\n",
    "                    sum_neighbors = array_in[i-1, j] + array_in[i, j+1] + array_in[i-1, j+1]\n",
    "                    avg = sum_neighbors/3\n",
    "                    \n",
    "                elif j == (n-1):\n",
    "                    #Lower right corner\n",
    "                    sum_neighbors = array_in[i, j-1] + array_in[i-1, j] + array_in[i-1, j-1]\n",
    "                    avg = sum_neighbors/3\n",
    "                    \n",
    "                else:\n",
    "                    #Lower edge except corners\n",
    "                    sum_neighbors = array_in[i, j-1] + array_in[i, j+1] + array_in[i-1, j] + array_in[i-1, j-1] + array_in[i-1, j+1]\n",
    "                    avg = sum_neighbors/5\n",
    "                    \n",
    "            else:\n",
    "                if j == 0:\n",
    "                    #Left edge except corners\n",
    "                    sum_neighbors = array_in[i-1, j] + array_in[i+1, j] + array_in[i, j+1] + array_in[i-1, j+1] + array_in[i+1, j+1]\n",
    "                    avg = sum_neighbors/5\n",
    "                    \n",
    "                elif j == (n-1):\n",
    "                    #Right edge except corners\n",
    "                    sum_neighbors = array_in[i-1, j] + array_in[i+1, j] + array_in[i, j-1] + array_in[i-1, j-1] + array_in[i+1, j-1]\n",
    "                    avg = sum_neighbors/5\n",
    "                    \n",
    "                else:\n",
    "                    #Not on any edge or corner\n",
    "                    sum_neighbors = array_in[i, j+1] + array_in[i, j-1] + array_in[i-1, j] + array_in[i+1, j] + \\\n",
    "                                    array_in[i-1, j-1] + array_in[i-1, j+1] + array_in[i+1, j-1] + array_in[i+1, j+1]\n",
    "                    avg = sum_neighbors/8\n",
    "                    \n",
    "                    \n",
    "            array_out[i,j] = avg\n",
    "            #/for loop body\n",
    "        \n",
    "    return array_out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b861d4d7c9deadde",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test the function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4193a15c763f7b70"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Test the function from above:\n",
    "import numpy as np\n",
    "arr = [[1,2,3], \n",
    "       [4,5,6],\n",
    "       [7,8,9]]\n",
    "arr = np.array(arr)\n",
    "arr_avgs = avg_neighbors(arr)\n",
    "print(arr_avgs)\n",
    "#Good: appears to work"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fb301367df9b44c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure the function works with tensors."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce4c3c8482427623"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Let's experiment to see if I know what's going on in the code cell 35 above:\n",
    "ls = ['Staring', 'at', 'the', 'tiny', 'planet', 'God', 'calculated', 'again']\n",
    "ls_iter = iter(ls)\n",
    "for i in range (len(ls)):\n",
    "    print(next(ls_iter))\n",
    "    \n",
    "#Ok, so far so good\n",
    "ls2 = [['There', 'was'], ['no', 'room'], ['for', 'a'], ['continuous', 'forest']]\n",
    "ls2_iter = iter(ls2)\n",
    "for i in range (len(ls2)):\n",
    "    first_word, second_word = next(ls2_iter)\n",
    "    print(first_word, second_word)\n",
    "#Yes, ok, working as expected"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "933e6c82f43ef21c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Try code on previous fire masks. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "def4b11b10d332e5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Let's try using the function above on the previous fire masks:\n",
    "#This code (is supposed to) find the first \"interesting\" previous fire mask and compute the avg number of neighboring \n",
    "#on-fire cells for each cell\n",
    "\n",
    "prev_fire_masks = inputs[:, :, :, 11] #observation number, pixel row, pixel col\n",
    "\n",
    "found_it_flag = 0 #will set to 1 once we've found our \"interesting\" fire mask\n",
    "img_num = 0\n",
    "while found_it_flag == 0:\n",
    "    fire_mask = np.array(prev_fire_masks[img_num, :, :])\n",
    "    if (np.all( (fire_mask == 0)) ): #if boring picture where there's no fire, toss it\n",
    "        img_num = img_num + 1\n",
    "    elif (np.all( np.invert(fire_mask == -1) )): #if NO data is missing data, cond. is TRUE --> you want this one\n",
    "        test_img = fire_mask\n",
    "        found_it_flag = 1\n",
    "    else:\n",
    "        img_num = img_num + 1\n",
    "\n",
    "np.set_printoptions(threshold=np.inf) #this just stops Jupyter from truncating the output\n",
    "print('fire mask:\\n', fire_mask, '\\n\\n')\n",
    "print('computed avg neighbor fire mask:\\n', avg_neighbors(fire_mask))\n",
    "#Note: don't freak out when you see the second matrix \"look bigger\" than the first. The dimensions are correct; the second matrix\n",
    "#is just visually larger because instead of 0s and 1s its entries are 3fs.\n",
    "#Tested with small grids (9x9) and appears to work."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9f844c531f0bc92",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next few cells, eliminate missing or uncertain data in previous fire masks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d31ade4cfa894391"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Try to eliminate all observations where there are uncertain squares in the previous fire mask.\n",
    "prev_masks_array = np.array(inputs[:, :, :, 11])\n",
    "#print(prev_masks_array.shape) #100x32x32—good!\n",
    "\n",
    "#Build the array of certain data AND SAVE THE INDICES\n",
    "first_find_flag = 1\n",
    "count = 0\n",
    "indices = []\n",
    "\n",
    "for img_num in range(num_obs): \n",
    "    fire_mask = np.array(prev_fire_masks[img_num, :, :])\n",
    "    if (np.all( np.invert(fire_mask == -1) )): #If no missing data, condition is TRUE.\n",
    "        count += 1\n",
    "        indices.append(img_num)\n",
    "        if first_find_flag == 1: #If you need to start the array\n",
    "            certain_prev_fire_masks = fire_mask\n",
    "            first_find_flag = 0  #Remember to turn the flag off!\n",
    "        else:\n",
    "            certain_prev_fire_masks = np.dstack((certain_prev_fire_masks, fire_mask)) #d\n",
    "            \n",
    "\n",
    "#Test: You want the printouts from the following three lines to be consistent\n",
    "print(certain_prev_fire_masks.shape)\n",
    "print(count) \n",
    "print(len(indices))\n",
    "#Good, they are."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2796e55e514502d2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay next code. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e7cc72569e6bbc8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Now that I've found the observations with \"pristine\" previous fire masks, I'd like to get those observations' features matrices\n",
    "full_input_array = np.array(inputs) #100x32x32x12\n",
    "\n",
    "for i, index in enumerate(indices):\n",
    "    if i == 0:\n",
    "        certain_input_array = full_input_array[index,:,:,:]\n",
    "        print(certain_input_array.shape) #32x32x12\n",
    "    elif i == 1:\n",
    "        certain_input_array = np.concatenate((certain_input_array[..., np.newaxis], full_input_array[index,:,:,:, np.newaxis]), axis=3)\n",
    "    else:\n",
    "        certain_input_array = np.concatenate((certain_input_array, full_input_array[index,:,:,:, np.newaxis]), axis=3)\n",
    "        \n",
    "print(certain_input_array.shape)\n",
    "#SUCCESS! :) :) :) \n",
    "#certain_input_array now holds only the 77 observations with certain previous fire masks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ff863085495461",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "More code. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "383c3b8fa9d75cac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Also want the labels at only these indices and the average neighbor value matrices\n",
    "full_labels = np.array(labels)\n",
    "#print(labels.shape) 100x32x32x1, as expected\n",
    "\n",
    "for i, index in enumerate(indices):\n",
    "    if i == 0:\n",
    "        #labels\n",
    "        certain_labels = full_labels[index,:,:,:] #32x32x1\n",
    "        \n",
    "        #avg neighbor values\n",
    "        surrounding_fire_scores = avg_neighbors(full_labels[index,:,:,:])\n",
    "        surrounding_fire_scores = surrounding_fire_scores[..., np.newaxis]\n",
    "        \n",
    "    else:\n",
    "        #labels\n",
    "        certain_labels = np.concatenate((certain_labels, full_labels[index,:,:,:]), axis=2)\n",
    "        \n",
    "        #avg neighbor values\n",
    "        avg_mat = avg_neighbors(full_labels[index,:,:,:])\n",
    "        surrounding_fire_scores = np.concatenate((surrounding_fire_scores, avg_mat[...,np.newaxis]), axis=2)\n",
    "\n",
    "#Printouts from following lines should agree...\n",
    "print(certain_labels.shape) #32x32x77 \n",
    "print(surrounding_fire_scores.shape) #32x32x77 \n",
    "#... and they do :)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f03df4dfed4528e9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even more code. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9a1ea9ba8f71e31"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Function to identify certain observations in the previous fire mask and return:\n",
    "#1) a list of their indices in the batch (need this to grab the right ones from the labels) and \n",
    "#2) the actual array of certain observations\n",
    "\n",
    "def elim_uncertain(prev_fire_mask_batch):\n",
    "    \n",
    "    prev_masks_array = np.array(prev_fire_mask_batch)\n",
    "    num_imgs, rows, cols = prev_masks_array.shape\n",
    "    \n",
    "    #Build the array of certain data AND SAVE THE INDICES\n",
    "    first_find_flag = 1\n",
    "    count = 0\n",
    "    indices = []\n",
    "\n",
    "    for img_num in range(num_imgs): \n",
    "        fire_mask = prev_fire_mask_batch[img_num, :, :] #grab the \"working fire mask\" off the pile\n",
    "        \n",
    "        if (np.all( np.invert(fire_mask == -1) )): #If no missing data, condition is TRUE.\n",
    "            count += 1\n",
    "            indices.append(img_num)\n",
    "            if first_find_flag == 1: #If you need to start the array\n",
    "                certain_prev_fire_masks_batch = fire_mask\n",
    "                first_find_flag = 0  #Remember to turn the flag off!\n",
    "            else:\n",
    "                certain_prev_fire_masks_batch = np.dstack((certain_prev_fire_masks_batch, fire_mask)) \n",
    "    \n",
    "    return certain_prev_fire_masks_batch, indices"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10c20ab700a9bc60",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "More and more code. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20376ebb0c59344"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Function to extract only the labels (i.e. current fire masks) from the certain observations\n",
    "def extract_certain_labels(certain_indices, og_labels):\n",
    "    \n",
    "    for i, index in enumerate(certain_indices):\n",
    "        if i == 0:\n",
    "            extracted_labels = og_labels[index,:,:,:] #the og_labels dimensions are batch_size by sidelength by sidelength by 1\n",
    "        else:\n",
    "            #labels\n",
    "            extracted_labels = np.concatenate((extracted_labels, og_labels[index,:,:,:]), axis=2)\n",
    "\n",
    "    return extracted_labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8871c21396a5178e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even more and more code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cc36bff56c978a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Create multi-D array of neighbor fire values\n",
    "def avg_neighbor_batch(batch_in):\n",
    "    rows, cols, batch_size = batch_in.shape #ordering of dimensions here meant to be compatible with elim_uncertain and extract_certain_labels\n",
    "    batch_out = np.zeros((rows, cols, batch_size))\n",
    "    for i in range(batch_size):\n",
    "        working_arr = batch_in[:,:,i]\n",
    "        avgd_arr = avg_neighbors(working_arr)\n",
    "        batch_out[:,:,i] =  avgd_arr\n",
    "    #/for loop\n",
    "    \n",
    "    return batch_out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8091dfb628064867",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moooree code. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa7b18060561746e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Test functions above:\n",
    "import numpy as np\n",
    "full_prev_fire_masks = np.array(inputs[:,:,:,11]) #just grabs the previous fire masks from the full observation multi-D array\n",
    "full_curr_fire_masks = np.array(labels) #100x32x32x1\n",
    "\n",
    "certain_prev_masks, certain_indices = elim_uncertain(full_prev_fire_masks) #32x32x86 (generally: sidelength^2 x #certain obs.)\n",
    "certain_labels = extract_certain_labels(certain_indices, full_curr_fire_masks) #32x32x86 (generally: sidelength^2 x #certain obs.)\n",
    "avg_neighbors_feat = avg_neighbor_batch(certain_prev_masks) #32x32x86 (generally: sidelength^2 x #certain obs.)\n",
    "\n",
    "#Success! Note that new format for certain_labels and certain_prev_masks is rows, cols, index in (new, certain-only) pile."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b71420f2dad169c1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even moooree code. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e80567653886c6b2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Benchmark method: Try linear regression based on only:\n",
    "#1) whether a given pixel is currently on fire and \n",
    "#2) number of neighbors that are currently on fire\n",
    "\n",
    "#Vectorize previous fire masks\n",
    "flat_prev_masks = []\n",
    "\n",
    "#Remembering that Python is zero-indexed is going to be VERY IMPORTANT in the below!\n",
    "for obs in range(certain_prev_masks.shape[2]):\n",
    "    for row in range(certain_prev_masks.shape[0]):\n",
    "        for col in range(certain_prev_masks.shape[1]):\n",
    "            flat_prev_masks.append(certain_prev_masks[row, col, obs])\n",
    "flat_prev_masks = np.array(flat_prev_masks)\n",
    "\n",
    "#Vectorize avg neighbors\n",
    "flat_avg_nbrs = []\n",
    "for obs in range(avg_neighbors_feat.shape[2]):\n",
    "    for row in range(avg_neighbors_feat.shape[0]):\n",
    "        for col in range(avg_neighbors_feat.shape[1]):\n",
    "            flat_avg_nbrs.append(avg_neighbors_feat[row, col, obs])\n",
    "flat_avg_nbrs = np.array(flat_avg_nbrs)\n",
    "\n",
    "            \n",
    "#print(flat_prev_masks.shape)\n",
    "#print(flat_avg_nbrs.shape)\n",
    "\n",
    "X_train = np.vstack((np.transpose(flat_prev_masks), np.transpose(flat_avg_nbrs))) \n",
    "X_train = np.transpose(X_train) #so that observations correspond to rows now\n",
    "\n",
    "#Vectorize labels\n",
    "flat_labels = []\n",
    "for obs in range(certain_labels.shape[2]):\n",
    "    for row in range(certain_labels.shape[0]):\n",
    "        for col in range(certain_labels.shape[1]):\n",
    "            flat_labels.append(certain_labels[row, col, obs])\n",
    "flat_labels = np.array(flat_labels)\n",
    "\n",
    "Y_train = flat_labels\n",
    "#print(Y_train.shape)\n",
    "\n",
    "variables = [\"currently on fire?\", \"# of neighbors currently on fire\"]\n",
    "\n",
    "#Regression time!\n",
    "#Note that I do NOT want to standardize in this simple case because the units are the same for the two inputs (#pix on fire)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lr_fire = LinearRegression().fit(X_train, Y_train)\n",
    "\n",
    "print('training R2-score:', np.round(   r2_score(  Y_train, lr_fire.predict(X_train)),2  )   ) \n",
    "\n",
    "scores_fire = cross_val_score(lr_fire, X_train, Y_train, cv=10, scoring='r2')\n",
    "\n",
    "print(\"validation R2-scores:\",np.round(scores_fire,2))\n",
    "print(\"average:\", np.round(np.mean(scores_fire),2))\n",
    "\n",
    "print('LR coefficients:')\n",
    "for i, coeff in enumerate(lr_fire.coef_):\n",
    "    print('{0:5s}  {1:>-10.2f}'.format(variables[i], np.round(coeff,2)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcb3b8bc14ff967d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The plotting function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc1c03fc979693d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define names of variables."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba074d338f6ba807"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "TITLES = [\n",
    "  'Elevation',\n",
    "  'Wind\\ndirection',\n",
    "  'Wind\\nvelocity',\n",
    "  'Min\\ntemp',\n",
    "  'Max\\ntemp',\n",
    "  'Humidity',\n",
    "  'Precip',\n",
    "  'Drought',\n",
    "  'Vegetation',\n",
    "  'Population\\ndensity',\n",
    "  'Energy\\nrelease\\ncomponent',\n",
    "  'Previous\\nfire\\nmask',\n",
    "  'Fire\\nmask'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a07b992c1e5ab083",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper variables to plot. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46a909b6cb60c104"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Number of rows of data samples to plot\n",
    "n_rows = 5 \n",
    "# Number of data variables\n",
    "n_features = inputs.shape[3]\n",
    "# Variables for controllong the color map for the fire masks\n",
    "CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
    "BOUNDS = [-1, -0.1, 0.001, 1]\n",
    "NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dfb0a1ca94b9afd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,6.5))\n",
    "\n",
    "for i in range(n_rows):\n",
    "  for j in range(n_features + 1):\n",
    "    plt.subplot(n_rows, n_features + 1, i * (n_features + 1) + j + 1)\n",
    "    if i == 0:\n",
    "      plt.title(TITLES[j], fontsize=13)\n",
    "    if j < n_features - 1:\n",
    "      plt.imshow(inputs[i, :, :, j], cmap='viridis')\n",
    "    if j == n_features - 1:\n",
    "      plt.imshow(inputs[i, :, :, -1], cmap=CMAP, norm=NORM)\n",
    "    if j == n_features:\n",
    "      plt.imshow(labels[i, :, :, 0], cmap=CMAP, norm=NORM) \n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f06871251acc21a9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2a0d32964021e373"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
